# 🎯 Learning from Experience 上級: 完全ブラインド最適化学習システム

**システム概要**: 完全に未知の2次元目的関数を効率的に最小化するため、過去の評価履歴のみを活用して段階的に最適化戦略を学習・改善する真のブラインド探索システム

**重要**: この最適化では、目的関数の形も最適解の位置も一切知ることができません。外部プログラムから返される関数値のみを頼りに、純粋にデータ駆動の推論で最適解を発見する必要があります。

**学習目標**:
- 完全ブラインド環境での戦略的探索
- 評価履歴のみからの論理的推論
- 真のデータ駆動最適化思考の習得

---

## 完全初期化（クリーンスタート）

variables.jsonが存在すれば削除してください
TODOリストをすべてクリアしてください

「=== 完全ブラインド最適化学習システム 処理開始 ===」と表示してください。

## Phase 1: ブラインド最適化問題の設定

今回の最適化対象は完全に未知の関数です：

**目的関数**: 秘密の2次元関数 f(x,y)
**定義域**: x,y ∈ [-5, 5]
**制約**: 関数評価は最大10回まで
**目標**: f(x,y)の最小値を効率的に発見
**評価方法**: 外部プログラム `python3 objective_function.py x y` のみ

**重要な制約**: 
- 関数の形は一切不明
- 最適解の位置も不明
- 過去の評価結果のみから推論すること

---

## Phase 2: 最適化履歴の初期化

以下の構造でoptimization_history.jsonファイルを作成し、最適化履歴を初期化してください：

```json
{
  "optimization_history": [],
  "current_best": {
    "x": null,
    "y": null,
    "f_xy": null,
    "iteration": null
  },
  "strategy_notes": []
}
```

評価回数を0として{{evaluation_count}}に設定してください。

---

## Phase 3: ブラインド最適化ループの実行

以下の停止条件のいずれかに達するまで、以下を繰り返してください：

**停止条件**:
1. {{evaluation_count}}が10回に達した場合
2. 連続2回の評価で目的関数の改善が0.1以下の場合

{{evaluation_count}}が10回に達するか、早期停止条件を満たすまで繰り返してください：

### 候補点の選択
{{evaluation_count}}に1を加算してください。

「=== 最適化回数 {{evaluation_count}} ===」と表示してください。

optimization_history.jsonを読み込んで{{opt_history}}に設定してください。

{{opt_history}}を参照して、次の候補点(x,y)を決定してください：

初回（{{evaluation_count}} = 1）の場合：
→ 定義域x,y ∈ [-5, 5]内で初期候補点を選択してください（例：初期点(-2.0, -2.0)）

2回目以降の場合：
→ {{opt_history}}の内容を分析し、**以下を明確に説明した上で**次の候補点を決定してください：
  - 過去のどのデータから何を学習したか
  - なぜその候補点が有望と判断したか  
  - どのような仮説・戦略に基づく選択か
  - 探索vs活用のどちらを重視するか

選択した候補点を{{current_x}}, {{current_y}}に設定し、「候補点: ({{current_x}}, {{current_y}})」と表示してください。

### 目的関数の評価（ブラインド評価）
以下のコマンドを実行して関数値を取得してください：

```bash
python3 objective_function.py {{current_x}} {{current_y}}
```

出力された値を{{current_f_xy}}に設定してください。

「評価結果: f({{current_x}}, {{current_y}}) = {{current_f_xy}}」と表示してください。

### 履歴の更新
評価結果を{{opt_history}}に追加し、optimization_history.jsonを更新してください：

```json
{
  "iteration": "{{evaluation_count}}",
  "x": "{{current_x}}",
  "y": "{{current_y}}",
  "f_xy": "{{current_f_xy}}",
  "strategy_used": "あなたが使用した選択戦略の説明",
  "insights": "この評価から得られた気づき"
}
```

現在の最良値（最小のf_xy）を確認し、更新が必要であればcurrent_bestを更新してください。

### 戦略仮説の検証と学習
評価結果について以下を分析し、strategy_notesに記録してください：
- 事前に立てた仮説が正しかったか
- この結果から新たに学習したパターン
- 関数の性質について推測できること
- 次回選択のための戦略調整内容
- 探索vs活用のバランス判断

### 早期停止条件の確認
現在の{{evaluation_count}}回目の評価後、以下を確認してください：

**連続改善チェック**:
- 前回の最良値と今回の最良値の差を計算
- 前々回の最良値と前回の最良値の差を計算
- 両方の改善が0.1以下の場合、「連続2回で改善が0.1以下」として早期停止を実行

早期停止条件を満たした場合は「=== 早期停止条件達成 ===」と表示してPhase 4に進んでください。

---

## Phase 4: ブラインド最適化結果の分析

最適化ループ完了後、以下の分析を実行してください：

### 最終結果の評価
{{opt_history}}から最適化結果を分析してください：

- **発見した最小値**: current_bestの値
- **最適(x,y)座標**: 最小値を与える(x,y)
- **効率性評価**: 最大10回の評価（または早期停止）でどの程度良い解を発見できたか

### ブラインド学習プロセスの進化分析
{{opt_history}}の推論・学習プロセスの変遷を分析してください：

- **初期段階の推論**: 最初の数回でどのような論理的判断を行ったか
- **中期の学習**: 履歴データから何を学習し、戦略をどう調整したか
- **終盤の最適化**: 蓄積された知見をどう活用し、精密化を図ったか

### 関数の性質推定
評価結果から関数の性質について推定してください：

- **大まかな傾向**: どのような形状の関数だと推測されるか
- **最適解の特徴**: 発見した最適解周辺の特性
- **探索戦略の有効性**: どの戦略が最も効果的だったか

### 探索vs活用のバランス
最適化プロセスにおける探索（exploration）と活用（exploitation）のバランスを分析してください：

- **探索フェーズ**: 未知領域の調査に重点を置いた段階
- **活用フェーズ**: 有望領域での精密探索に移行した段階
- **バランス調整**: 両者のバランスをどのように調整したか

---

## Phase 5: 学習成果の総括

### 習得したブラインド最適化技法
この経験から学習した効果的な推論・学習手法を整理してください：

- **データ解釈技法**: 限られた評価結果からパターンを読み取る方法
- **仮説検証手法**: 予測を立てて結果で検証するプロセス
- **戦略調整法**: 新しい情報に基づく戦略修正のアプローチ

### 汎用的ブラインド学習指針
この経験から、一般的なブラインド最適化問題に適用できる指針を作成してください：

- **段階的学習戦略**: 各段階での効果的な学習アプローチ
- **履歴データ活用**: 過去経験の効果的な分析・推論方法
- **不確実性下の意思決定**: 情報が限られた状況での効果的な選択手法

---

## 学習完了

「完全ブラインド最適化学習システム完了！」と表示し、以下について振り返ってください：

- **学習性能**: 発見した解の品質と推論効率性
- **思考進化**: ブラインド環境でのデータ駆動思考の発達過程
- **応用可能性**: 他の未知最適化問題への適用展望

## システムクリーンアップ

次回実行時のために、optimization_history.jsonファイルを削除してください。
これにより、次回実行時に新しい最適化学習を純粋に体験できます。